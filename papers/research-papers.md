## Research Papers
List of research papers, linguistic tools and resources for low-resource natural language processing (LR-NLP).
<details>
<summary><b>Formal Grammar and Linguistic Analysis</b></summary>

| Title | Authors | Year | Link |
|-------|---------|------|------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Vaswani et al. | 2017 | [PDF](https://arxiv.org/pdf/1706.03762.pdf) |
| [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) | Devlin et al. | 2018 | [PDF](https://arxiv.org/pdf/1810.04805.pdf) |

</details>

<details>
<summary><b>Foundation papers - transformers and pre-trained models</b></summary>

| Title | Authors | Year | Link |
|-------|---------|------|------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Vaswani et al. | 2017 | [PDF](https://arxiv.org/pdf/1706.03762.pdf) |
| [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) | Devlin et al. | 2018 | [PDF](https://arxiv.org/pdf/1810.04805.pdf) |

</details>


<details>
<summary><b>Papers with resusable resources including datasets</b></summary>

| Title | Authors | Year | Link |
|-------|---------|------|------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Vaswani et al. | 2017 | [PDF](https://arxiv.org/pdf/1706.03762.pdf) |
| [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) | Devlin et al. | 2018 | [PDF](https://arxiv.org/pdf/1810.04805.pdf) |

</details>


<details>
<summary><b>LR-NLP Tools and Resources</b></summary>

| Title | Authors | Year | Link |
|-------|---------|------|------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Vaswani et al. | 2017 | [PDF](https://arxiv.org/pdf/1706.03762.pdf) |
| [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) | Devlin et al. | 2018 | [PDF](https://arxiv.org/pdf/1810.04805.pdf) |

</details>


<details>
<summary><b>Downstream Tasks</b></summary>

| Title | Authors | Year | Link |
|-------|---------|------|------|
| [Deep Learning for Sentiment Analysis](https://arxiv.org/abs/1408.5882) | Kim, | 2014 | [PDF](https://arxiv.org/abs/1408.5882) |
| [---](---) | --- | --- | [PDF](---) |

</details>


<details>
<summary><b>Embedding and Pretrained Models</b></summary>

| Title | Authors | Year | Link |
|-------|---------|------|------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Vaswani et al. | 2017 | [PDF](https://arxiv.org/pdf/1706.03762.pdf) |
| [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) | Devlin et al. | 2018 | [PDF](https://arxiv.org/pdf/1810.04805.pdf) |

</details>

<details>
<summary><b>Enrichment Strategy for LR-NLP</b></summary>

| Title | Authors | Year | Link |
|-------|---------|------|------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Vaswani et al. | 2017 | [PDF](https://arxiv.org/pdf/1706.03762.pdf) |
| [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) | Devlin et al. | 2018 | [PDF](https://arxiv.org/pdf/1810.04805.pdf) |

</details>


