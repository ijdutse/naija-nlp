## Research Papers
### Transformers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al., 2017.
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Devlin et al., 2018.

### Sentiment Analysis
- [Deep Learning for Sentiment Analysis](https://arxiv.org/abs/1408.5882) - Kim, 2014.




## Transformers and Pre-trained Models

| Title | Authors | Year | Link |
|-------|---------|------|------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | Vaswani et al. | 2017 | [PDF](https://arxiv.org/pdf/1706.03762.pdf) |
| [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) | Devlin et al. | 2018 | [PDF](https://arxiv.org/pdf/1810.04805.pdf) |
| [T5: Exploring the Limits of Transfer Learning](https://arxiv.org/abs/1910.10683) | Raffel et al. | 2019 | [PDF](https://arxiv.org/pdf/1910.10683.pdf) |


